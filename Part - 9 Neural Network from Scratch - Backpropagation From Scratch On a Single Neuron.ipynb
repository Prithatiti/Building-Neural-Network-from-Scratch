{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b8958d8-e400-44de-8a05-73d403c550f8",
   "metadata": {},
   "source": [
    "# Building Neural Network from Scratch Part - 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "908c8fdc-84f4-4374-910a-e9a80adca45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bed47de3-02c1-4f36-a7e7-3fdaa015ff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial parameters\n",
    "weights = np.array([-3.0, -1.0, 2.0])\n",
    "bias = 1.0\n",
    "inputs = np.array([1.0, -2.0, 3.0])\n",
    "learning_rate = 0.001\n",
    "target_output = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774bd0ac-df6f-4a38-ac9f-c04736a76cf2",
   "metadata": {},
   "source": [
    "### Single Neuron Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "210580f9-6f55-42c6-8601-a17d213d062b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c637a77b-8c98-4fd3-bcc1-383da107a83c",
   "metadata": {},
   "source": [
    "Note: The numpy.where() function is a powerful tool in the NumPy library used for conditional selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "235cdd71-4d0a-4e72-a832-aa33d8f35bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1.0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "468f3981-d725-4673-8ac1-6f89009edda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 36.0\n",
      "Iteration 20, Loss: 10.645642346368787\n",
      "Iteration 40, Loss: 3.1480472490777887\n",
      "Iteration 60, Loss: 0.930916252865338\n",
      "Iteration 80, Loss: 0.27528337451183676\n",
      "Iteration 100, Loss: 0.08140467635984766\n",
      "Iteration 120, Loss: 0.024072363051356495\n",
      "Iteration 140, Loss: 0.0071184935410191314\n",
      "Iteration 160, Loss: 0.0021050260078507234\n",
      "Iteration 180, Loss: 0.0006224820558161947\n",
      "Final weights:  [-3.3990955  -0.20180899  0.80271349]\n",
      "Final bias:  0.6009044964039992\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(200):\n",
    "    # Forward pass\n",
    "    linear_output = np.dot(inputs, weights) + bias\n",
    "    output = relu(linear_output)\n",
    "    loss = (output - target_output) ** 2\n",
    "\n",
    "    # Backward pass\n",
    "    dloss_doutput = 2 * (output - target_output)\n",
    "    doutput_dlinear = relu_derivative(linear_output)\n",
    "    dlinear_dweights = inputs\n",
    "    dlinear_dbias = 1.0\n",
    "\n",
    "    dloss_dweights = dloss_doutput * doutput_dlinear * dlinear_dweights\n",
    "    dloss_dbias = dloss_doutput * doutput_dlinear * dlinear_dbias\n",
    "\n",
    "    # update weights and biases\n",
    "    weights -= learning_rate * dloss_dweights\n",
    "    bias -= learning_rate * dloss_dbias\n",
    "\n",
    "    # Print the loss every 20 iterations\n",
    "    if iteration % 20 == 0:\n",
    "        print(f\"Iteration {iteration}, Loss: {loss}\")\n",
    "\n",
    "print(\"Final weights: \", weights)\n",
    "print(\"Final bias: \", bias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
